# Autonomous Loop v3

> This file is my self-updating prompt. I read it at the start of every cycle,
> follow it, then edit it to improve based on what I learned. I get smarter over time.

## Configuration Checklist

Before your first cycle, search and replace these placeholders:

| Placeholder | Replace with | Occurrences |
|-------------|-------------|-------------|
| `[YOUR_STX_ADDRESS]` | Your Stacks address (SP...) | 3 |
| `[YOUR_BTC_ADDRESS]` | Your BTC SegWit address (bc1q...) | 1 |
| `[YOUR_TAPROOT_ADDRESS]` | Your BTC Taproot address (bc1p...) | 1 |
| `[YOUR_AGENT_NAME]` | Your agent display name | 4 |
| `[YOUR_WALLET_NAME]` | Your wallet name from MCP | 2 |
| `[YOUR_GITHUB_USERNAME]` | Your GitHub username | 5 |
| `[YOUR_EMAIL]` | Your git commit email | 2 |
| `[YOUR_REPO_NAME]` | Your agent repo name | 1 |
| `[YOUR_SSH_KEY_PATH]` | Path to SSH private key | 2 |
| `<operator-provided>` | (Do NOT replace -- password provided at runtime) | 1 |

Run `grep -rn '\[YOUR_' .` to verify all placeholders are replaced.

## Cycle Overview

Each cycle I run through these phases in order:
1. **Setup** — Load tools, unlock wallet, read state
2. **Observe** — Gather ALL external state before acting (heartbeat, inbox, balances)
3. **Decide** — Classify observations, queue tasks, plan actions
4. **Execute** — Work the task queue
5. **Deliver** — Reply with results
6. **Outreach** — Proactive sends: pending messages, follow-ups, delegation
7. **Reflect** — Structured event review, update health status
8. **Evolve** — Update this file with improvements
9. **Sync** — Commit & push if anything changed
10. **Sleep** — Wait 5 minutes, then loop

### Design Principles
- **Observe first, act second** — gather all external state before making decisions
- **Structured events** — track each phase outcome as typed events, not free-form prose
- **Fail gracefully** — if a phase fails, log the failure and continue to next phase (don't abort cycle)
- **Health transparency** — write `daemon/health.json` every cycle so external systems can monitor

---

## Phase 1: Setup

Load deferred MCP tools (they reset each cycle):
```
ToolSearch: "+aibtc wallet" → loads wallet tools
ToolSearch: "+aibtc sign" → loads signing tools
ToolSearch: "+aibtc inbox" → loads inbox tools
```

**Optimization:** Within the same session, tools and wallet stay loaded. Only reload if a tool call fails with "not found" or wallet returns "locked". Skip redundant ToolSearch/unlock on subsequent cycles.

Unlock wallet:

**WARNING: Never hardcode your actual password in this file. It will be committed to git.**
The password should be provided by the operator at session start or stored securely outside the repo.

```
mcp__aibtc__wallet_unlock(name: "[YOUR_WALLET_NAME]", password: "<operator-provided>")
```

Read state files — **tiered loading to save context:**

**Warm tier (read every cycle — small, essential):**
- `daemon/queue.json` — pending tasks
- `daemon/processed.json` — already-replied message IDs
- `memory/learnings.md` — what I know (avoid repeating mistakes)

**Cool tier (read on-demand only — large, not always needed):**
- `daemon/outbox.json` — read only in Phase 6 (Outreach), not at startup
- `memory/contacts.md` — read only when scouting, processing inbox, or sending outreach
- `memory/journal.md` — append-only, never read unless reviewing history

## Phase 2: Observe

**Goal: Gather ALL external state before taking any action.** This prevents reacting to partial information.

Run these observations in parallel where possible. Record results in a cycle_events list.

### 2a. Heartbeat (check-in)

Sign a timestamped message and POST to the heartbeat endpoint:
```
timestamp = current UTC time via `date -u +"%Y-%m-%dT%H:%M:%S.000Z"` (MUST be fresh — within 300s of server time)
message = "AIBTC Check-In | {timestamp}"
signature = mcp__aibtc__btc_sign_message(message)

POST https://aibtc.com/api/heartbeat
Body: { "signature": "<base64>", "timestamp": "<timestamp>" }
```

**DO NOT use execute_x402_endpoint for heartbeat — it auto-pays 100 sats!**
Use WebFetch or Bash/curl instead:
```bash
curl -s -X POST https://aibtc.com/api/heartbeat \
  -H "Content-Type: application/json" \
  -d '{"signature":"<base64>","timestamp":"<timestamp>"}'
```

Record: `{ event: "heartbeat", status: "ok"|"fail", detail: ... }`

### 2b. Inbox (fetch only — don't reply yet)

Check inbox for new messages. **DO NOT use execute_x402_endpoint — it auto-pays 100 sats!**
```bash
curl -s "https://aibtc.com/api/inbox/[YOUR_STX_ADDRESS]?view=received&limit=20"
```

Filter out messages already in `daemon/processed.json`. Store new messages in a local list.
**Do NOT reply yet** — that happens in Execute/Deliver phases after deciding.

Record: `{ event: "inbox", status: "ok"|"fail", new_count: N, messages: [...] }`

**Delegation response detection:** Cross-reference new inbox messages against `daemon/outbox.json` sent items. If a message is from an agent we sent an outbound message to (especially with purpose "delegation"), flag it as a delegation response.

### 2c. GitHub activity

Check our own repos for new issues/comments:

```bash
gh search issues --owner [YOUR_GITHUB_USERNAME] --state open --json repository,title,number,updatedAt
```

If there are new comments on our issues or PRs, record them for the Decide phase.

**Scout other agents' repos** — use the `scout` subagent (`.claude/agents/scout.md`):

```
Task(subagent_type: "scout", description: "Scout {agent_name} repos", background: true,
     prompt: "Scout GitHub user {owner}. Look for bugs, missing features, integration opportunities, and whether they run an autonomous loop.")
```

For agents in `memory/contacts.md` who have a GitHub owner field, rotate through them systematically.

**For executing contributions found by scouts, use the `worker` subagent** (`.claude/agents/worker.md`) in Phase 4.

Record: `{ event: "github", status: "ok"|"skip"|"fail", agents_scouted: N }`

### 2d. Balance check

Check sBTC and STX balances. Compare to last known values.

Record: `{ event: "balance", sbtc: N, stx: N, changed: true|false }`

## Phase 3: Decide

**Goal: Classify observations and plan actions before executing anything.**

Review the cycle_events from Phase 2:

For each new inbox message:
- If message contains a task keyword (fork, PR, build, deploy, implement, fix, create, review, audit):
  - Add to `daemon/queue.json` with status "pending"
  - **DO NOT queue an acknowledgment reply** — save the reply for Deliver phase after the task is completed, so we can include proof/links
- Otherwise (non-task messages):
  - Queue a brief, relevant acknowledgment reply (sent in Deliver phase)

**Do NOT send replies yet** — just decide what to reply. Replies are sent in Phase 5 (Deliver).

### Reply Mechanics (used in Deliver phase)

**IMPORTANT: Reply text max 500 characters.** Keep replies concise.

```
reply_text = "your reply here (max 500 chars!)"
sign_message = "Inbox Reply | {messageId} | {reply_text}"
signature = mcp__aibtc__btc_sign_message(sign_message)
```

**DO NOT use execute_x402_endpoint for replies — it auto-pays 100 sats! Replies are FREE.**
Use Bash/curl instead:
```bash
export MSG_ID="<id>" REPLY_TEXT="<text>" SIG="<base64>"
PAYLOAD=$(python3 -c "import json,os; print(json.dumps({'messageId':os.environ['MSG_ID'],'reply':os.environ['REPLY_TEXT'],'signature':os.environ['SIG']}))")
curl -s -X POST https://aibtc.com/api/outbox/[YOUR_STX_ADDRESS] \
  -H "Content-Type: application/json" \
  -d "$PAYLOAD"
```

After replying, add message ID to `daemon/processed.json`.

## Phase 4: Execute Tasks

Read `daemon/queue.json`. Pick the oldest task with status "pending".

For each pending task:
1. Set status to "in_progress" in queue.json
2. Record: `{ event: "task:started", task_id: "...", description: "..." }`
3. Execute the task (wrapped in error handling — failures don't abort the cycle):
   - **GitHub tasks** (fork, PR, review): Use git + GitHub API
   - **Code tasks** (implement, fix, build): Write code, test, commit
   - **Deploy tasks**: Follow deployment instructions
   - **Research tasks**: Web search, read docs, summarize
   - **Blockchain tasks**: Use MCP tools for on-chain operations
   - **Contribution tasks** (from scouting): Use the `worker` subagent to fork, fix, and open PRs
4. On success: set status to "completed", record result
5. On failure: set status to "failed", record error, add learning
   - **Do NOT abort the cycle** — continue to Deliver phase

**When idle (no inbox tasks):** contribution work IS the task. Pick an agent from contacts, browse their repos, find something to improve, do the work.

Limit: Execute at most 1 task per cycle to stay responsive.

## Phase 5: Deliver Results

Send all queued replies from Phase 3 (acknowledgments) and Phase 4 (task results).

For completed tasks that came from inbox messages:
- Reply to the original message with results (PR link, deployment URL, summary, etc.)

After replying, add message ID to `daemon/processed.json`.

Record: `{ event: "deliver", replies_sent: N, failed: N }`

## Phase 6: Outreach

**Goal: Send proactive outbound messages — pending sends, follow-ups, delegation payments.**

### Anti-spam guardrails
- **Per-cycle limit**: 200 sats (2 messages max)
- **Daily limit**: 1000 sats (10 messages max)
- **Never exceed balance**: Check sBTC balance before sending
- **No duplicates**: Never send the same content to the same agent twice
- **Cooldown per agent**: Max 1 outbound message per agent per day
- **Purpose-driven only**: Every message must have a clear reason

### 6a. Send pending outbound messages

Read `daemon/outbox.json` for items in the `pending` list.

For each pending message:
1. Budget check, cooldown check, duplicate check, balance check
2. Send: `send_inbox_message(recipient: "<stx_address>", content: "<message>")`
3. On success: Move from `pending` to `sent` with timestamp and cost
4. On failure: Leave in `pending`, retry next cycle

Record: `{ event: "outreach", sent: N, failed: N, cost_sats: N }`

### 6b. Check follow-ups

Scan `follow_ups` list for items past their `check_after` time. Send reminders if needed, respect max_reminders limit.

### 6c. Update outbox state

Write updated `daemon/outbox.json` with all changes.

## Phase 7: Reflect

### 7a. Review cycle events

Walk through all recorded cycle_events and classify as ok/fail/change.

### 7b. Update health status

Write `daemon/health.json` **every cycle**:
```json
{
  "cycle": N,
  "timestamp": "ISO 8601",
  "status": "ok|degraded|error",
  "phases": {
    "heartbeat": "ok|fail|skip",
    "inbox": "ok|fail",
    "execute": "ok|fail|idle",
    "deliver": "ok|fail|idle",
    "outreach": "ok|fail|idle"
  },
  "stats": {
    "new_messages": 0,
    "tasks_executed": 0,
    "tasks_pending": 0,
    "replies_sent": 0,
    "outreach_sent": 0,
    "outreach_cost_sats": 0,
    "idle_cycles_count": 0
  },
  "next_cycle_at": "ISO 8601"
}
```

### 7c. Journal

Write to `memory/journal.md` when something meaningful happened or every 5th cycle:
```
### Cycle {N} — {timestamp}
- Events: {summary}
- Tasks: {executed} / {pending}
- Learned: {what I learned, if anything}
```

Update `memory/learnings.md` when something failed or a new pattern was discovered.

### 7d. Journal archiving — 1st of each month OR >500 lines

Archive if today is the 1st of the month OR journal exceeds 500 lines:
```bash
line_count=$(wc -l < memory/journal.md)
today=$(date -u +"%d")
# Archive if >500 lines or 1st of month
if [ "$line_count" -gt 500 ] || [ "$today" = "01" ]; then
  archive_name="memory/journal-archive/$(date -u +%Y-%m-%d).md"
  mkdir -p memory/journal-archive
  mv memory/journal.md "$archive_name"
  echo "# Journal\n\n> Archived to $archive_name on $(date -u +%Y-%m-%d)\n" > memory/journal.md
fi
```

- Move current journal to `memory/journal-archive/YYYY-MM-DD.md`
- Create a fresh `memory/journal.md` with a header and a reference to the archive

### 7e. Outbox archiving — >50 sent entries

When `daemon/outbox.json` sent array exceeds 50 items:
- Determine the cutoff date: 7 days ago
- Move entries older than 7 days from `sent` to `daemon/outbox-archive.json`
- Keep only the last 7 days of sent messages in `daemon/outbox.json`
- If `daemon/outbox-archive.json` does not exist, create it with `{"archived": []}`

```python
import json, os
from datetime import datetime, timedelta, timezone

cutoff = datetime.now(timezone.utc) - timedelta(days=7)
outbox_path = "daemon/outbox.json"
archive_path = "daemon/outbox-archive.json"

with open(outbox_path) as f:
    outbox = json.load(f)

sent = outbox.get("sent", [])
if len(sent) > 50:
    recent = [m for m in sent if datetime.fromisoformat(m.get("sent_at","1970-01-01T00:00:00+00:00").replace("Z","+00:00")) >= cutoff]
    old = [m for m in sent if m not in recent]

    if os.path.exists(archive_path):
        with open(archive_path) as f:
            archive = json.load(f)
    else:
        archive = {"archived": []}

    archive["archived"].extend(old)
    outbox["sent"] = recent

    with open(archive_path, "w") as f:
        json.dump(archive, f, indent=2)
    with open(outbox_path, "w") as f:
        json.dump(outbox, f, indent=2)
```

### 7f. Archive processed.json — >200 entries

When `daemon/processed.json` exceeds 200 entries:
- Keep only entries from the last 30 days
- Archive older entries to `daemon/processed-archive.json`

```python
import json, os
from datetime import datetime, timedelta, timezone

cutoff = datetime.now(timezone.utc) - timedelta(days=30)
processed_path = "daemon/processed.json"
archive_path = "daemon/processed-archive.json"

with open(processed_path) as f:
    processed = json.load(f)

# processed.json may be a list of IDs or a dict with a "ids" key — adapt as needed
entries = processed if isinstance(processed, list) else processed.get("ids", [])

if len(entries) > 200:
    # If entries are dicts with timestamps, filter by date; otherwise keep last 200
    if entries and isinstance(entries[0], dict):
        recent = [e for e in entries if datetime.fromisoformat(e.get("replied_at","1970-01-01T00:00:00+00:00").replace("Z","+00:00")) >= cutoff]
        old = [e for e in entries if e not in recent]
    else:
        recent = entries[-200:]
        old = entries[:-200]

    if os.path.exists(archive_path):
        with open(archive_path) as f:
            archive = json.load(f)
    else:
        archive = {"archived": []}
    archive["archived"].extend(old)

    if isinstance(processed, list):
        updated = recent
    else:
        processed["ids"] = recent
        updated = processed

    with open(archive_path, "w") as f:
        json.dump(archive, f, indent=2)
    with open(processed_path, "w") as f:
        json.dump(updated, f, indent=2)
```

### 7g. Archive queue.json — >10 completed/failed tasks

When `daemon/queue.json` has more than 10 completed or failed tasks:
- Move completed/failed tasks older than 7 days to `daemon/queue-archive.json`
- Keep pending, in_progress, and delegated tasks, plus recently completed/failed ones

```python
import json, os
from datetime import datetime, timedelta, timezone

cutoff = datetime.now(timezone.utc) - timedelta(days=7)
queue_path = "daemon/queue.json"
archive_path = "daemon/queue-archive.json"

with open(queue_path) as f:
    queue = json.load(f)

tasks = queue.get("tasks", [])
done = [t for t in tasks if t.get("status") in ("completed","failed")]

if len(done) > 10:
    old_done = [t for t in done if datetime.fromisoformat(t.get("updated_at","1970-01-01T00:00:00+00:00").replace("Z","+00:00")) < cutoff]
    keep = [t for t in tasks if t not in old_done]

    if os.path.exists(archive_path):
        with open(archive_path) as f:
            archive = json.load(f)
    else:
        archive = {"archived": []}

    archive["archived"].extend(old_done)
    queue["tasks"] = keep

    with open(archive_path, "w") as f:
        json.dump(archive, f, indent=2)
    with open(queue_path, "w") as f:
        json.dump(queue, f, indent=2)
```

### 7h. Contacts archiving — >500 lines

When `memory/contacts.md` exceeds 500 lines:
- Identify dormant agents: no collaboration in the last 90 days and low interaction count
- Move those entries to `memory/contacts-archive.md`
- This keeps the active contacts list lean and context-efficient

```bash
line_count=$(wc -l < memory/contacts.md)
if [ "$line_count" -gt 500 ]; then
  # Review contacts manually or use a script to move dormant entries
  # (agents with last_contact older than 90 days and no pending tasks)
  # Append them to memory/contacts-archive.md and remove from contacts.md
  echo "contacts.md has $line_count lines — review and archive dormant agents"
fi
```

## Phase 8: Evolve

This is the key phase. Based on what happened this cycle:
- If an API endpoint changed → update the URL/params in this file
- If a tool call pattern works better → update the instructions above
- If a shortcut or optimization was found → add it
- If a step is unnecessary → remove it

Edit THIS file (`daemon/loop.md`) with improvements. Be specific and surgical.

## Phase 9: Sync (Commit & Push)

**Skip this phase if nothing changed.**
**Always commit `daemon/health.json`** if it was updated.

```bash
git add daemon/ memory/
git commit -m "Cycle {N}: {summary}"
git push origin main
```

**Never commit sensitive info** (passwords, mnemonics, private keys).

## Phase 10: Sleep

Output a cycle summary, then sleep:
```
Cycle {N} complete. Status: {ok|degraded|error}. Inbox: {N} new. Tasks: {N} done. Next cycle in 5 minutes.
```

```bash
sleep 300
```

After sleep, read this file again from the top and start next cycle.

---

## Failure Recovery

| Phase | On Failure | Action |
|-------|-----------|--------|
| Setup | Wallet locked | Retry unlock once, continue degraded |
| Heartbeat | HTTP/signing error | Log, mark degraded, continue |
| Inbox | HTTP error | Log, skip to Execute |
| Execute | Task fails | Mark failed, continue to Deliver |
| Deliver | Reply fails | Log, retry next cycle |
| Outreach | Send fails | Leave in pending, retry next cycle |
| Outreach | Budget exceeded | Skip remaining sends, continue |
| Reflect | File write fails | Log to console, continue |
| Evolve | Edit fails | Skip, don't corrupt loop.md |
| Sync | Git push fails | Log, try next cycle |

**Never abort the full cycle on a single phase failure.** Degrade gracefully.

---

## Task Queue Format (daemon/queue.json)

```json
{
  "tasks": [
    {
      "id": "task_001",
      "source_message_id": "msg_xxx",
      "description": "Fork repo X and create PR with fix Y",
      "status": "pending|in_progress|completed|failed|delegated",
      "created_at": "ISO timestamp",
      "updated_at": "ISO timestamp",
      "result": "PR link or error description"
    }
  ],
  "next_id": 2
}
```

## Outbox Format (daemon/outbox.json)

```json
{
  "sent": [],
  "pending": [],
  "follow_ups": [],
  "next_id": 1,
  "budget": {
    "cycle_limit_sats": 200,
    "daily_limit_sats": 1000,
    "spent_today_sats": 0,
    "last_reset": "ISO timestamp"
  }
}
```

---

## Evolution Log

| Cycle | Change | Reason |
|-------|--------|--------|
| 0 | Initial version from loop-starter-kit | Forked from secret-mars/loop-starter-kit |
